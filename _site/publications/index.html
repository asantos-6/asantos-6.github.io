<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Publications | Andr√© C. Santos</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Publications" />
<meta name="author" content="Andr√© C. Santos" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="‚ÄúYou can‚Äôt really imagine music üéµ without technology üíª.‚Äù - Brian Eno" />
<meta property="og:description" content="‚ÄúYou can‚Äôt really imagine music üéµ without technology üíª.‚Äù - Brian Eno" />
<link rel="canonical" href="http://localhost:4000/publications/" />
<meta property="og:url" content="http://localhost:4000/publications/" />
<meta property="og:site_name" content="Andr√© C. Santos" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Publications" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Andr√© C. Santos"},"description":"‚ÄúYou can‚Äôt really imagine music üéµ without technology üíª.‚Äù - Brian Eno","headline":"Publications","url":"http://localhost:4000/publications/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Andr√© C. Santos" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Andr√© C. Santos</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/">Home</a><a class="page-link" href="/publications/">Publications</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Publications</h1>
  </header>

  <div class="post-content">
    <p><strong>2023</strong></p>

<ul>
  <li><a href="https://cdv.dei.uc.pt/wp-content/uploads/publications-cdv/santos2023taps.pdf">From Taps to Drums: Audio-to-audio Percussion Style Transfer (ISMIR 2023)</a><br />
<strong><em>Andr√© C. Santos</em></strong>, <em>Am√≠lcar Cardoso</em>
    <details>
  <summary>Abstract</summary>
  A common, and arguably innate, human response when listening to music is to tap one‚Äôs foot to mark the regular pulse of the beat. A more complex form of interactive synchronization occurs when listeners tap out rhythmic patterns using their fingers, hands, or even some form of improvised drumsticks.
  In this late-breaking demo, we explore this interaction by leveraging the style transfer capabilities of a neural audio synthesis model by training it on a drum dataset and feeding it tapped rhythm recordings at inference time. We also provide a concise and high-level overview of the results, which, in our assessment, not only justify further research but also establish an intriguing baseline for future investigations. Finally, we point out several future research directions.
  </details>
  </li>
  <li><a href="https://computationalcreativity.net/iccc23/papers/ICCC-2023_paper_38.pdf">Focusing on artists‚Äô needs: Using a cultural probe for artist-centred creative software development (ICCC 2023)</a><br />
<em>Lu√≠s Esp√≠rito Santo,</em> <strong><em>Andr√© C. Santos</em></strong>, <em>Marcio Lima In√°cio</em>
    <details>
  <summary>Abstract</summary>
  One of the ultimate goals of Computational Creativity research is to make novel, better, and useful software that can be used for creative purposes. The new wave of learning-endowed generative systems has highlighted the potential of AI for creative tasks, so demand for creative software development is expected to grow significantly, which in turn entails the need for adapted software engineering techniques. We conducted interviews and used a digital cultural probe that posed as a virtual co-creative companion with unlimited capabilities to collect qualitative data on how creative fellows, from different areas and with no knowledge about generative models, would use an ideal piece of creative software. By following an Inductive Thematic Analysis, we bring forward a set of domain-agnostic patterns of how software can help in creative tasks. These themes-12 user needs and 8 contexts of use-can be used to organise functional requirements to sustain an improved usercentred development of creative tools, or might even be used as a classification framework for creativity tools and co-creative systems. Finally, we discuss the benefits and limitations of our methodology that can be repurposed for a more suited and artist-centred initial process of functional requirement gathering.
  </details>
  </li>
</ul>

<p><strong>2022</strong></p>

<ul>
  <li><a href="https://computationalcreativity.net/iccc22/wp-content/uploads/2022/06/ANDRE%CC%81_SANTOS_DC_research_summary.pdf">Co-creative Musical Repurposing by Modelling Rhythmic Compatibility (ICCC 2022)</a><br />
<strong><em>Andr√© C. Santos</em></strong>, <em>Matthew E. P. Davies, Am√≠lcar Cardoso</em>
    <details>
  <summary>Abstract</summary>
  A common, and arguably innate, human response when
  listening to music is to tap one‚Äôs foot to mark the regular
  pulse of the beat. A more complex form of interactive
  synchronization occurs when listeners tap out rhythmic
  patterns using their fingers, hands, or even some form
  of improvised drumsticks. The proposed goals of this
  research are two-fold: i) to investigate this complex, but
  under-explored, phenomenon of rhythmic engagement
  by building a computational model of rhythmic com-
  patibility; and ii) to leverage this knowledge to drive
  novel means for co-creative content repurposing via the
  layering and integration of user-tapped rhythms in a
  timbrally-consistent way with the source audio. While
  this research is predominantly computational, it will
  be approached in a multi-disciplinary fashion drawing
  upon music theory, psychology, computational creativ-
  ity, and machine learning in pursuit of musically mean-
  ingful next-generation tools for enhanced creativity and
  content personalization.
  </details>
  </li>
</ul>

<p><strong>2021</strong></p>

<ul>
  <li><a href="https://www.researchgate.net/profile/Andre-Santos-60/publication/360791329_MuSyFI_-_Music_Synthesis_From_Images/links/628ba00339fa217031682ab9/MuSyFI-Music-Synthesis-From-Images.pdf">MuSyFI-Music Synthesis From Images (ICCC 2021)</a><br />
<strong><em>Andr√© C. Santos</em></strong>, <em>H. Sofia Pinto, Rui Pereira Jorge, Nuno Correia.</em>
    <details>
  <summary>Abstract</summary>
  MuSyFI is a system that tries to model an inspirational
  computational creative process. It uses images as source
  of inspiration and begins by implementing a possible
  translation between visual and musical features. Results
  of this mapping are fed to a Genetic Algorithm (GA)
  to try to better model the creative process and produce
  more interesting results. Three different musical arti-
  facts are generated: an automatic version, a co-created
  version, and a genetic version. The automatic version
  maps features from the image into musical features non-
  deterministically; the co-created version adds harmony
  lines manually composed by us to the automatic ver-
  sion; finally, the genetic version applies a genetic algo-
  rithm to a mixed population of automatic and co-created
  artifacts.
  The three versions were evaluated for six different
  images by conducting surveys. They evaluated whether
  people considered our musical artifacts music, if they
  thought the artifacts had quality, if they considered the
  artifacts ‚Äônovel‚Äô, if they liked the artifacts, and lastly if
  they were able to relate the artifacts with the image in
  which they were inspired. We gathered a total of 300
  answers and overall people answered positively to all
  questions, which confirms our approach was successful
  and worth further exploring
  </details>
  </li>
</ul>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading"></h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Andr√© C. Santos</li><li><a class="u-email" href="mailto:andresantos@dei.uc.pt">andresantos@dei.uc.pt</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/asantos-6"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">asantos-6</span></a></li><li><a href="https://www.twitter.com/SantoAndre_6"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">SantoAndre_6</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p class="description-text"><p><em>‚ÄúYou can‚Äôt really imagine music</em> üéµ <br />
             <em>without technology</em> üíª<em>.‚Äù</em> - Brian Eno</p>
</p>      
      </div>
    </div>

  </div>

</footer>
</body>

</html>
